{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1488536",
   "metadata": {},
   "source": [
    "# <center>`Gradient Descent`</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01879499",
   "metadata": {},
   "source": [
    "The linear regression model is given by:  \n",
    "y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + ... + β<sub>n</sub>x<sub>n</sub>  \n",
    "where,   \n",
    "x<sub>1</sub> + x<sub>2</sub> + ... + x<sub>n</sub> are n features    \n",
    "β<sub>1</sub> + β<sub>2</sub> + ... + β<sub>n</sub> are the coeffients (weights)  \n",
    "β<sub>0</sub> is the intercept   \n",
    "The β terms are called the parameters  \n",
    "y is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79262a36",
   "metadata": {},
   "source": [
    "<b>Core goal of Linear Regression model:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d7718",
   "metadata": {},
   "source": [
    "Find the values of β<sub>0</sub>, β<sub>1</sub>, β<sub>2</sub>, ... , β<sub>n</sub>  \n",
    "such that the error (cost/loss) is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a0c75",
   "metadata": {},
   "source": [
    "<b>Mean Squared Error:</b>  \n",
    "Mean Squared Error is given by:  \n",
    "MSE = $\\frac{1}{N} \\sum \\limits _{i=1} ^{N}$ (y<sub>i</sub> - $\\hat{y}$<sub>i</sub>)<sup>2</sup>  \n",
    "where,  \n",
    "y<sub>i</sub> is the labeled output  \n",
    "$\\hat{y}$<sub>i</sub> is the predicted output  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7affb19d",
   "metadata": {},
   "source": [
    "Assuming we have only 1 feature,  \n",
    "$\\hat{y}$<sub>i</sub> = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub>  \n",
    "\n",
    "Substituting the value of $\\hat{y}$<sub>i</sub> in MSE equation to get minimum MSE:  \n",
    "MSE = $\\frac{1}{N} \\sum \\limits _{i=1} ^{N}$ [y<sub>i</sub> - (β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub>)]<sup>2</sup>  \n",
    "\n",
    "in order to minimize the MSE, we can only bring changes to β<sub>0</sub> & β<sub>1</sub> which represent the real continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8ad8e",
   "metadata": {},
   "source": [
    "<b>What must be the values of β<sub>0</sub> & β<sub>1</sub> so that MSE becomes minimum?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf85cc",
   "metadata": {},
   "source": [
    "Lets set β<sub>0</sub> as constant.  \n",
    "Tracking the value of loss(error) with various values of β<sub>1</sub> ...\n",
    "\n",
    "<img src='graph.png' width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74acb432",
   "metadata": {},
   "source": [
    "<b>Steps to find β<sub>1</sub>:</b>  \n",
    "\n",
    "1. Randomly initializing the values of β<sub>0</sub> & β<sub>1</sub>.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Find the value of $\\hat{y}$ i.e. (β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub>).\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Calculate the MSE.  \n",
    "   i.e. MSE = L = $\\frac{1}{N} \\sum \\limits _{i=1} ^{N}$ (y<sub>i</sub> - $\\hat{y}$<sub>i</sub>)<sup>2</sup> \n",
    "\n",
    "<br>\n",
    "\n",
    "4. Find the value of gradient.  \n",
    "   $\\frac{\\delta\\ L}{\\delta\\ \\beta1}$ : gradient w.r.to β<sub>1</sub>  \n",
    "   $\\frac{\\delta\\ L}{\\delta\\ \\beta0}$ : gradient w.r.to β<sub>0</sub>  \n",
    "\n",
    "<br>\n",
    "\n",
    "5. Update Rule:  \n",
    "   β<sub>0</sub> = β<sub>0</sub> - $\\eta$ $\\frac{\\delta\\ L}{\\delta\\ \\beta0}$  \n",
    "   β<sub>1</sub> = β<sub>1</sub> - $\\eta$ $\\frac{\\delta\\ L}{\\delta\\ \\beta1}$  \n",
    "   where,  \n",
    "   $\\eta$ = (0.1 to 0.001) is the learning rate\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "The steps 2 to 5 are iterated over and over again, so that the initial Loss and initial $\\beta$<sub>1</sub> are reduced to the  minimum loss and best $\\beta$<sub>1</sub> values (global minima) in the graph.  \n",
    "\n",
    "Here, $\\eta$ is called the hyperparameter because we are required to set the value of $\\eta$ from (0.1 to 0.001) during the training of our Machine Learning algorithm.  \n",
    "\n",
    "The partial derivative of parameters β<sub>0</sub> & β<sub>1</sub> are:  \n",
    "$\\frac{\\delta\\ L}{\\delta\\ \\beta0}$ = $\\frac{-2}{N} \\sum \\limits _{i=1} ^{N}$ [y<sub>i</sub> - β<sub>0</sub> - β<sub>1</sub>x<sub>1</sub>] \n",
    "= $\\frac{-2}{N} \\sum \\limits _{i=1} ^{N}$ (y<sub>i</sub> - $\\hat{y}$<sub>i</sub>)  \n",
    "\n",
    "Similarly,  \n",
    "\n",
    "$\\frac{\\delta\\ L}{\\delta\\ \\beta1}$ = $\\frac{-2}{N} \\sum \\limits _{i=1} ^{N}$ [y<sub>i</sub> - β<sub>0</sub> - β<sub>1</sub>x<sub>1</sub>] x<sub>i</sub> \n",
    "= $\\frac{-2}{N} \\sum \\limits _{i=1} ^{N}$ (y<sub>i</sub> - $\\hat{y}$<sub>i</sub>) x<sub>i</sub> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
